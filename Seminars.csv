Date,Name,Institution,Title,Abstract,Room,Hours,Meet
08/11/2024,Junhyung Park,Tubingen,Causal Spaces: A measure-theoretic axiomatisation of causality,"We view causality both as an extension of probability theory and as a study of what happens when one intervenes on a system, and argue in favour of taking Kolmogorov's measure-theoretic axiomatisation of probability as the starting point towards an axiomatisation of causality. To that end, we propose the notion of a causal space, consisting of a probability space along with a collection of transition probability kernels, called causal kernels, that encode the causal information of the space. Our proposed framework is not only rigorously grounded in measure theory, but it also sheds light on long-standing limitations of existing frameworks including, for example, cycles, latent variables and stochastic processes.",aaaa
18/10/2024,Alberto Caron,,Causal Spaces: A measure-theoretic axiomatisation of causality,"We view causality both as an extension of probability theory and as a study of what happens when one intervenes on a system, and argue in favour of taking Kolmogorov's measure-theoretic axiomatisation of probability as the starting point towards an axiomatisation of causality. To that end, we propose the notion of a causal space, consisting of a probability space along with a collection of transition probability kernels, called causal kernels, that encode the causal information of the space. Our proposed framework is not only rigorously grounded in measure theory, but it also sheds light on long-standing limitations of existing frameworks including, for example, cycles, latent variables and stochastic processes.",aaaa
05/10/2024,Isacco Beretta and Martina Cinquini,Università di Pisa,A Practical Approach to Causal Inference over Time,"In this paper, we focus on estimating the causal effect of an intervention over time on a dynamical system. To that end, we formally define causal interventions and their effects over time on discrete-time stochastic processes (DSPs). Then, we show under which conditions the equilibrium states of a DSP, both before and after a causal intervention, can be captured by a structural causal model (SCM). With such an equivalence at hand, we provide an explicit mapping from vector autoregressive models (VARs), broadly applied in econometrics, to linear, but potentially cyclic and/or affected by unmeasured confounders, SCMs. The resulting causal VAR framework allows us to perform causal inference over time from observational time series data. Our experiments on synthetic and real-world datasets show that the proposed framework achieves strong performance in terms of observational forecasting while enabling accurate estimation of the causal effect of interventions on dynamical systems. We demonstrate, through a case study, the potential practical questions that can be addressed using the proposed causal VAR framework.",Sala Demo,15:00-16:00,https://teams.microsoft.com/l/meetup-join/19%3a5neLefsT-A7pgtfUzYE2JxJKjuGOaU-_Paof_ts3S1o1%40thread.tacv2/1721644338288?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%228108ed0e-93bf-4848-97ef-e1de8d1d75e9%22%7d
26/7/2024,Gabriele Dominici and Pietro Barbiero,Università della Svizzera Italiana,Causal Concept Embedding Models Beyond Causal Opacity in Deep Learning,"Causal opacity denotes the difficulty in understanding the "hidden" causal structure underlying a deep neural network's (DNN) reasoning. This leads to the inability to rely on and verify state-of-the-art DNN-based systems especially in high-stakes scenarios. For this reason, causal opacity represents a key open challenge at the intersection of deep learning, interpretability, and causality. This work addresses this gap by introducing Causal Concept Embedding Models (Causal CEMs), a class of interpretable models whose decision-making process is causally transparent by design. The results of our experiments show that Causal CEMs can: (i) match the generalization performance of causally-opaque models, (ii) support the analysis of interventional and counterfactual scenarios, thereby improving the model's causal interpretability and supporting the effective verification of its reliability and fairness, and (iii) enable human-in-the-loop corrections to mispredicted intermediate reasoning steps, boosting not just downstream accuracy after corrections but also accuracy of the explanation provided for a specific instance.",Sala Demo,15:00-16:00,https://teams.microsoft.com/l/meetup-join/19%3a5neLefsT-A7pgtfUzYE2JxJKjuGOaU-_Paof_ts3S1o1%40thread.tacv2/1721644338288?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%228108ed0e-93bf-4848-97ef-e1de8d1d75e9%22%7d
10/7/2024,Alessio Moneta,Scuola Superiore Sant'Anna,Vector Autoregressions Identification and Causality,"The structural vector autoregressive (SVAR) model is the main tool that macroeconomists use to address empirical questions about the effects of policy interventions. I will review the SVAR model, making comparisons with the structural causal model, and I will address the question whether it is equipped to warrant causal claims. Specifically, I will present the model identification problem and spot the difference with the problem of causal inference.",Sala Demo,14:00-15:00,https://teams.microsoft.com/l/meetup-join/19%3a5neLefsT-A7pgtfUzYE2JxJKjuGOaU-_Paof_ts3S1o1%40thread.tacv2/1720515380224?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%22c12d9249-9ea3-41b4-8fcb-36d83f7cf49e%22%7d
28/6/2024,Jose Manuel Alvarez,Scuola Normale Superiore,Causal Perception,"Perception occurs when two individuals interpret the same information differently. Despite being a known phenomenon with implications for bias in decision-making, as individual experience determines interpretation, perception remains largely overlooked in machine learning (ML) research. Modern decision flows, whether partially or fully automated, involve human experts interacting with ML applications. How might we then, e.g., account for two experts that interpret differently a deferred instance or an explanation from a ML model? To account for perception, we first need to formulate it. In this work, we define perception under causal reasoning using structural causal models (SCM). Our framework formalizes individual experience as additional causal knowledge that comes with and is used by a human expert (read, decision maker). We present two kinds of causal perception, unfaithful and inconsistent, based on the SCM properties of faithfulness and consistency. Further, we motivate the importance of perception within fairness problems. We illustrate our framework through a series of decision flow examples involving ML applications and human experts.",Sala Demo,15:00-16:00,https://teams.microsoft.com/l/meetup-join/19%3a5neLefsT-A7pgtfUzYE2JxJKjuGOaU-_Paof_ts3S1o1%40thread.tacv2/1719561923516?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%224dd426ce-e9eb-4f68-a979-2751471049d0%22%7d
21/6/2024,Riccardo Massidda,University of Pisa,Learning Linear Causal Abstraction,"The need for modelling causal knowledge at different levels of granularity arises in several settings. Causal Abstraction provides a framework for formalizing this problem by relating two Structural Causal Models at different levels of detail. Despite increasing interest in applying causal abstraction, e.g. in the interpretability of large machine learning models, the graphical and parametrical conditions under which a causal model can abstract another are not known. Furthermore, learning causal abstractions from data is still an open problem. In this work, we tackle both issues for linear causal models with linear abstraction functions. First, we characterize how the low-level coefficients and the abstraction function determine the high-level coefficients and how the high-level model constrains the causal ordering of low-level variables. Then, we apply our theoretical results to learn high-level and low-level causal models and their abstraction function from observational data. In particular, we introduce Abs-LiNGAM, a method that leverages the constraints induced by the learned high-level model and the abstraction function to speedup the recovery of the larger low-level model, under the assumption of non-Gaussian noise terms. In simulated settings, we show the effectiveness of learning causal abstractions from data and the potential of our method in improving scalability of causal discovery.",Sala Demo,15:00-16:00,https://teams.microsoft.com/l/meetup-join/19%3a5neLefsT-A7pgtfUzYE2JxJKjuGOaU-_Paof_ts3S1o1%40thread.tacv2/1718097875089?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%224dd426ce-e9eb-4f68-a979-2751471049d0%22%7d
07/6/2024,Andrea Pugnana,University of Pisa,A causal framework for evaluating deferring systems,"Deferring systems extend supervised Machine Learning (ML) models with the possibility to defer predictions to human experts. However, evaluating the impact of a deferring strategy on system accuracy is still an overlooked area. This paper fills this gap by evaluating deferring systems through a causal lens. We link the potential outcomes framework for causal inference with deferring systems. This allows us to identify the causal impact of the deferring strategy on predictive accuracy. We distinguish two scenarios. In the first one, we can access both the human and the ML model predictions for the deferred instances. In such a case, we can identify the individual causal effects for deferred instances and aggregates of them. In the second scenario, only human predictions are available for the deferred instances. In this case, we can resort to regression discontinuity design to estimate a local causal effect. We empirically evaluate our approach on synthetic and real datasets for seven deferring systems from the literature.",Sala Demo,15:00-16:00,https://teams.microsoft.com/l/meetup-join/19%3a5neLefsT-A7pgtfUzYE2JxJKjuGOaU-_Paof_ts3S1o1%40thread.tacv2/1717164844490?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%228108ed0e-93bf-4848-97ef-e1de8d1d75e9%22%7d
