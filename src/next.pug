.row.mt-4.mb-4
  h2 #[span.emoji ðŸ”®] Next Talks

.row.project
  .col-md-2.col-3
    h3.mb-0 18th
    h5.month.mb-0 October
    p 15:00-16:00
  .col-md-7.col-9
    p.author #[span.me Alberto Caron]
    h4.title.mb-1
      | TBD
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-40f4f4662f9022ee542d7df917ef4bdd",aria-expanded="false",aria-controls="talk-40f4f4662f9022ee542d7df917ef4bdd")
        | #[i.bi.bi-card-text] Abstract
  .col-md-7
    p.abstract#talk-40f4f4662f9022ee542d7df917ef4bdd.collapse.mt-2
        | TBD
.row.project
  .col-md-2.col-3
    h3.mb-0 25th
    h5.month.mb-0 October
    p 15:00-16:00
  .col-md-7.col-9
    p.author #[span.me Jacopo Bono]
    h4.title.mb-1
      | DiConStruct Causal Concept-based Explanations through Black-Box Distillation
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-734d80f7185e98ffc58a5c66d27b4907",aria-expanded="false",aria-controls="talk-734d80f7185e98ffc58a5c66d27b4907")
        | #[i.bi.bi-card-text] Abstract
      a(href="slides/DiConStruct Causal Concept-based Explanations through Black-Box Distillation.pdf",target="_blank").btn.btn-primary
        | #[i.bi.bi-file-earmark-text-fill] Paper
  .col-md-7
    p.abstract#talk-734d80f7185e98ffc58a5c66d27b4907.collapse.mt-2
        | Model interpretability plays a central role in human-AI decision-making systems. Ideally, explanations should be expressed using human-interpretable semantic concepts. Moreover, the causal relations between these concepts should be captured by the explainer to allow for reasoning about the explanations. Lastly, explanation methods should be efficient and not compromise the predictive task performance. Despite the recent rapid advances in AI explainability, as far as we know, no method yet fulfills these three desiderata. Indeed, mainstream methods for local concept explainability do not yield causal explanations and incur a trade-off between explainability and prediction accuracy. We present DiConStruct, an explanation method that is both concept-based and causal, which produces more interpretable local explanations in the form of structural causal models and concept attributions. Our explainer works as a distillation model to any black-box machine learning model by approximating its predictions while producing the respective explanations. Consequently, DiConStruct generates explanations efficiently while not impacting the black-box prediction task. We validate our method on an image dataset and a tabular dataset, showing that DiConStruct approximates the black-box models with higher fidelity than other concept explainability baselines, while providing explanations that include the causal relations between the concepts.
.row.project
  .col-md-2.col-3
    h3.mb-0 8th
    h5.month.mb-0 November
    p 15:00-16:00
  .col-md-7.col-9
    p.author #[span.me Junhyung Park]
    h4.title.mb-1
      | Causal Spaces A measure-theoretic axiomatisation of causality
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-94ec16d11c0b9c8de0fad620176ed030",aria-expanded="false",aria-controls="talk-94ec16d11c0b9c8de0fad620176ed030")
        | #[i.bi.bi-card-text] Abstract
      a(href="slides/Causal Spaces A measure-theoretic axiomatisation of causality.pdf",target="_blank").btn.btn-primary
        | #[i.bi.bi-file-earmark-text-fill] Paper
  .col-md-7
    p.abstract#talk-94ec16d11c0b9c8de0fad620176ed030.collapse.mt-2
        | We view causality both as an extension of probability theory and as a study of what happens when one intervenes on a system, and argue in favour of taking Kolmogorov's measure-theoretic axiomatisation of probability as the starting point towards an axiomatisation of causality. To that end, we propose the notion of a causal space, consisting of a probability space along with a collection of transition probability kernels, called causal kernels, that encode the causal information of the space. Our proposed framework is not only rigorously grounded in measure theory, but it also sheds light on long-standing limitations of existing frameworks including, for example, cycles, latent variables and stochastic processes.
.row.project
  .col-md-2.col-3
    h3.mb-0 15th
    h5.month.mb-0 November
    p 15:00-16:00
  .col-md-7.col-9
    p.author #[span.me Alexander Reisach]
    h4.title.mb-1
      | Sortability in Additive Noise Models
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-5d315bd4dfe6826ee76219cbfcb279c9",aria-expanded="false",aria-controls="talk-5d315bd4dfe6826ee76219cbfcb279c9")
        | #[i.bi.bi-card-text] Abstract
      a(href="slides/Sortability in Additive Noise Models.pdf",target="_blank").btn.btn-primary
        | #[i.bi.bi-file-earmark-text-fill] Paper
  .col-md-7
    p.abstract#talk-5d315bd4dfe6826ee76219cbfcb279c9.collapse.mt-2
        | Causal graphical models encode causal relationships between variables, typically based on a directed acyclic graph (DAG). Structural causal models expand upon this by expressing the causal relationships as explicit functions, which allows for computing the effect of interventions and much more. We show that, in many common parameterizations using linear functions and additive noise, effects accumulate along the causal order. This property enables inferring the causal order from data simply by sorting by a suitable criterion. We introduce the concept of sortability to capture the magnitude of the phenomenon, and show that the accumulating effects can lead to unrealistic values and deterministic relationships. We discuss implications for existing results and future research based on the same model class.
.row.project
  .col-md-2.col-3
    h3.mb-0 22nd
    h5.month.mb-0 November
    p  a critical problem is ensuring that machine learning models successfully encode the human concepts in their representations in a correct manner. Specifically
  .col-md-7.col-9
    p.author #[span.me Emanuale Marconato]
    h4.title.mb-1
      | A Causal framework for interpreting model representations
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-6067ba3aca8715ebb6027fd3b65fcf79",aria-expanded="false",aria-controls="talk-6067ba3aca8715ebb6027fd3b65fcf79")
        | #[i.bi.bi-card-text] Abstract
  .col-md-7
    p.abstract#talk-6067ba3aca8715ebb6027fd3b65fcf79.collapse.mt-2
        | There is a growing interest in providing explanations of machine learning models using human-understandable concepts. This integration of concepts can alleviate model opacity due to neural networks
.row.project
  .col-md-2.col-3
    h3.mb-0 29th
    h5.month.mb-0 November
    p 15:00-16:00
  .col-md-7.col-9
    p.author #[span.me Francesco Montagna]
    h4.title.mb-1
      | Lessons on the identifiability of causal models: classical and supervised learning approaches
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-38b6a72288fb1459d9b6835855d6ba61",aria-expanded="false",aria-controls="talk-38b6a72288fb1459d9b6835855d6ba61")
        | #[i.bi.bi-card-text] Abstract
  .col-md-7
    p.abstract#talk-38b6a72288fb1459d9b6835855d6ba61.collapse.mt-2
        | Causal discovery from observational data is hindered by restrictive modelling hypotheses that may be unrealistic and hard to verify. In this talk, we will illustrate how score matching estimation enables causal discovery with observational data on broad classes of causal models, with linear and nonlinear mechanisms, even in the presence of latent confounders. We draw the connection between our findings and the general principles that underlie most of the known results on the identifiability of causal models from observational data. Then, we turn our attention to supervised learning-based approaches for causal discovery: a plethora of learning algorithms for the amortized inference of causal graphs have been recently proposed, accompanied by impressive empirical results. Yet, it is unclear when the output of these methods can be trusted, as they do not come with clear identifiability guarantees. We show that amortized causal discovery still needs to obey identifiability theory, but it also differs from classical methods in how the assumptions are formulated, trading more reliance on assumptions on the noise type for fewer hypotheses on the mechanisms.