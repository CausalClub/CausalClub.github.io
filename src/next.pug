.row.mt-4.mb-4
  h2 #[span.emoji ðŸ”®] Next Talks

.row.project
  .col-md-2.col-3
    h3.mb-0 22nd
    h5.month.mb-0 November
    p 15:00-16:00
  .col-md-7.col-9
    p.author #[span.me Emanuale Marconato]
    h4.title.mb-1
      | A Causal framework for interpreting model representations
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-6067ba3aca8715ebb6027fd3b65fcf79",aria-expanded="false",aria-controls="talk-6067ba3aca8715ebb6027fd3b65fcf79")
        | #[i.bi.bi-card-text] Abstract
  .col-md-7
    p.abstract#talk-6067ba3aca8715ebb6027fd3b65fcf79.collapse.mt-2
        | There is a growing interest in providing explanations of machine learning models using human-understandable concepts. This integration of concepts can alleviate model opacity due to neural networks opening new venues to design interpretable and trustworthy models. However a critical problem is ensuring that machine learning models successfully encode the human concepts in their representations in a correct manner. Specifically when can we assign names to machine representations and how well do their concepts align with ours? I will demonstrate how Causality and Causal Representation Learning can help formalize this problem offering a clear definition of what can be referred to as human-machine alignment. Throughout the presentation I will showcase examples of both successes and failures in models designed to integrate concepts into their representations."
.row.project
  .col-md-2.col-3
    h3.mb-0 29th
    h5.month.mb-0 November
    p 15:00-16:00
  .col-md-7.col-9
    p.author #[span.me Francesco Montagna]
    h4.title.mb-1
      | Lessons on the identifiability of causal models: classical and supervised learning approaches
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-38b6a72288fb1459d9b6835855d6ba61",aria-expanded="false",aria-controls="talk-38b6a72288fb1459d9b6835855d6ba61")
        | #[i.bi.bi-card-text] Abstract
  .col-md-7
    p.abstract#talk-38b6a72288fb1459d9b6835855d6ba61.collapse.mt-2
        | Causal discovery from observational data is hindered by restrictive modelling hypotheses that may be unrealistic and hard to verify. In this talk, we will illustrate how score matching estimation enables causal discovery with observational data on broad classes of causal models, with linear and nonlinear mechanisms, even in the presence of latent confounders. We draw the connection between our findings and the general principles that underlie most of the known results on the identifiability of causal models from observational data. Then, we turn our attention to supervised learning-based approaches for causal discovery: a plethora of learning algorithms for the amortized inference of causal graphs have been recently proposed, accompanied by impressive empirical results. Yet, it is unclear when the output of these methods can be trusted, as they do not come with clear identifiability guarantees. We show that amortized causal discovery still needs to obey identifiability theory, but it also differs from classical methods in how the assumptions are formulated, trading more reliance on assumptions on the noise type for fewer hypotheses on the mechanisms.
.row.project
  .col-md-2.col-3
    h3.mb-0 6th
    h5.month.mb-0 December
    p Sala Demo
  .col-md-7.col-9
    p.author #[span.me Yorgos Felekis]
    h4.title.mb-1
      | Causal Optimal Transport of Abstractions
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-47a59c2589887c77ee1a6d0e1ee34d0f",aria-expanded="false",aria-controls="talk-47a59c2589887c77ee1a6d0e1ee34d0f")
        | #[i.bi.bi-card-text] Abstract
      a(href="slides/Causal Optimal Transport of Abstractions.pdf",target="_blank").btn.btn-primary
        | #[i.bi.bi-file-earmark-text-fill] Paper
  .col-md-7
    p.abstract#talk-47a59c2589887c77ee1a6d0e1ee34d0f.collapse.mt-2
        | makwCausal abstraction (CA) theory establishes formal criteria for relating multiple structural causal models (SCMs) at different levels of granularity by defining maps between them. These maps have significant relevance for real-world challenges such as synthesizing causal evidence from multiple experimental environments learning causally consistent representations at different resolutions and linking interventions across multiple SCMs. In this talk Yorgos will present COTA