.row.mt-4.mb-4
  h2 #[span.emoji ðŸ”®] Next Talks

.row.project
  .col-md-2.col-3
    h3.mb-0 18th
    h5.month.mb-0 October
    p 15:00-16:00
  .col-md-7.col-9
    p.author #[span.me Alberto Caron]
    h4.title.mb-1
      | TBD
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-40f4f4662f9022ee542d7df917ef4bdd",aria-expanded="false",aria-controls="talk-40f4f4662f9022ee542d7df917ef4bdd")
        | #[i.bi.bi-card-text] Abstract
  .col-md-7
    p.abstract#talk-40f4f4662f9022ee542d7df917ef4bdd.collapse.mt-2
        | TBD
.row.project
  .col-md-2.col-3
    h3.mb-0 25th
    h5.month.mb-0 October
    p 15:00-16:00
  .col-md-7.col-9
    p.author #[span.me Jacopo Bono]
    h4.title.mb-1
      | DiConStruct Causal Concept-based Explanations through Black-Box Distillation
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-734d80f7185e98ffc58a5c66d27b4907",aria-expanded="false",aria-controls="talk-734d80f7185e98ffc58a5c66d27b4907")
        | #[i.bi.bi-card-text] Abstract
      a(href="slides/DiConStruct Causal Concept-based Explanations through Black-Box Distillation.pdf",target="_blank").btn.btn-primary
        | #[i.bi.bi-file-earmark-text-fill] Paper
  .col-md-7
    p.abstract#talk-734d80f7185e98ffc58a5c66d27b4907.collapse.mt-2
        | Model interpretability plays a central role in human-AI decision-making systems. Ideally, explanations should be expressed using human-interpretable semantic concepts. Moreover, the causal relations between these concepts should be captured by the explainer to allow for reasoning about the explanations. Lastly, explanation methods should be efficient and not compromise the predictive task performance. Despite the recent rapid advances in AI explainability, as far as we know, no method yet fulfills these three desiderata. Indeed, mainstream methods for local concept explainability do not yield causal explanations and incur a trade-off between explainability and prediction accuracy. We present DiConStruct, an explanation method that is both concept-based and causal, which produces more interpretable local explanations in the form of structural causal models and concept attributions. Our explainer works as a distillation model to any black-box machine learning model by approximating its predictions while producing the respective explanations. Consequently, DiConStruct generates explanations efficiently while not impacting the black-box prediction task. We validate our method on an image dataset and a tabular dataset, showing that DiConStruct approximates the black-box models with higher fidelity than other concept explainability baselines, while providing explanations that include the causal relations between the concepts.
.row.project
  .col-md-2.col-3
    h3.mb-0 8th
    h5.month.mb-0 November
    p 15:00-16:00
  .col-md-7.col-9
    p.author #[span.me Junhyung Park]
    h4.title.mb-1
      | Causal Spaces A measure-theoretic axiomatisation of causality
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-94ec16d11c0b9c8de0fad620176ed030",aria-expanded="false",aria-controls="talk-94ec16d11c0b9c8de0fad620176ed030")
        | #[i.bi.bi-card-text] Abstract
      a(href="slides/Causal Spaces A measure-theoretic axiomatisation of causality.pdf",target="_blank").btn.btn-primary
        | #[i.bi.bi-file-earmark-text-fill] Paper
  .col-md-7
    p.abstract#talk-94ec16d11c0b9c8de0fad620176ed030.collapse.mt-2
        | We view causality both as an extension of probability theory and as a study of what happens when one intervenes on a system, and argue in favour of taking Kolmogorov's measure-theoretic axiomatisation of probability as the starting point towards an axiomatisation of causality. To that end, we propose the notion of a causal space, consisting of a probability space along with a collection of transition probability kernels, called causal kernels, that encode the causal information of the space. Our proposed framework is not only rigorously grounded in measure theory, but it also sheds light on long-standing limitations of existing frameworks including, for example, cycles, latent variables and stochastic processes.
.row.project
  .col-md-2.col-3
    h3.mb-0 15th
    h5.month.mb-0 November
    p 15:00-16:00
  .col-md-7.col-9
    p.author #[span.me Alexander Reisach]
    h4.title.mb-1
      | Sortability in Additive Noise Models
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-5d315bd4dfe6826ee76219cbfcb279c9",aria-expanded="false",aria-controls="talk-5d315bd4dfe6826ee76219cbfcb279c9")
        | #[i.bi.bi-card-text] Abstract
      a(href="slides/Sortability in Additive Noise Models.pdf",target="_blank").btn.btn-primary
        | #[i.bi.bi-file-earmark-text-fill] Paper
  .col-md-7
    p.abstract#talk-5d315bd4dfe6826ee76219cbfcb279c9.collapse.mt-2
        | Causal graphical models encode causal relationships between variables, typically based on a directed acyclic graph (DAG). Structural causal models expand upon this by expressing the causal relationships as explicit functions, which allows for computing the effect of interventions and much more. We show that, in many common parameterizations using linear functions and additive noise, effects accumulate along the causal order. This property enables inferring the causal order from data simply by sorting by a suitable criterion. We introduce the concept of sortability to capture the magnitude of the phenomenon, and show that the accumulating effects can lead to unrealistic values and deterministic relationships. We discuss implications for existing results and future research based on the same model class.