.row.mt-4.mb-2
  h2 #[span.emoji ðŸš€] Upcoming

.row.next
  .col-md-7.col-8
    p.author #[span.me Emanuale Marconato]
    h2.title.mb-0.mt-1
      | A Causal framework for interpreting model representations
  .col-md-2.col-4
    h1.day 22nd
    h4.month.mb-0 November
    p 15:00-16:00
  .col-md-7.col-12.mt-sm-3.mt-lg-1
      p
        | There is a growing interest in providing explanations of machine learning models using human-understandable concepts. This integration of concepts can alleviate model opacity due to neural networks opening new venues to design interpretable and trustworthy models. However a critical problem is ensuring that machine learning models successfully encode the human concepts in their representations in a correct manner. Specifically when can we assign names to machine representations and how well do their concepts align with ours? I will demonstrate how Causality and Causal Representation Learning can help formalize this problem offering a clear definition of what can be referred to as human-machine alignment. Throughout the presentation I will showcase examples of both successes and failures in models designed to integrate concepts into their representations.
  .col-md-4.d-grid.gap-2.d-md-block
    
    a(href="https://meet.google.com/rqt-auvp-drf").btn.btn-primary.mb-md-3.w-100
      | #[i.bi.bi-camera-reels-fill] Live Streaming
    a(href="https://goo.gl/maps/FL4qcbB3MnMXrYS28",target="_blank").btn.btn-primary.w-100
      | #[i.bi.bi-geo-alt-fill] Sala Demo
      