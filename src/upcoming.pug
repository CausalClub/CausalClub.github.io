.row.mt-4.mb-2
  h2 #[span.emoji üöÄ] Upcoming

.row.next
  .col-md-7.col-8
    p.author #[span.me Alberto Caron]
    h2.title.mb-0.mt-1
      | Causal Active Planning in Model-Based Reinforcement Learning
  .col-md-2.col-4
    h1.day 18th
    h4.month.mb-0 October
    p 15:00-16:00
  .col-md-7.col-12.mt-sm-3.mt-lg-1
      p
        | In this talk, I am going to present some results from our latest paper on the problem of sample efficient exploration in Model-Based Reinforcement Learning (MBRL). While most popular exploration methods in (MB)RL are ‚Äúreactive‚Äù in nature, and thus inherently sample inefficient, we discuss the benefits of an ‚Äúactive‚Äù approach, where the agent selects actions to query novel states in a data-efficient way, provided that one can guarantee that regions of high epistemic, and not aleatoric, uncertainty are targeted. In order to ensure this, we consider classes of exploration bonuses based on ‚ÄúBayesian surprise‚Äù, and demonstrate their desirable properties. The rest of the talk will be dedicated to presenting our latest, early stage, project that aims at extending this previous work to develop a ‚ÄúCausal Active Planning‚Äù framework for sample efficient and robust MBRL, where the goal is to expand this class of exploration bonuses to capture epistemic uncertainty around a (Local) Causal Graphical Model of the transition dynamics. By incorporating these measures as causal ‚Äúintrinsic rewards‚Äù within a MBRL algorithm, we can enable the agent to (actively) learn a causal transition dynamics model. If this causal dynamics model is then consequently utilized for policy training, it should theoretically ensure that the learnt policy is more robust and generalizable (Richens & Everitt, 2024).
  .col-md-4.d-grid.gap-2.d-md-block
    
    a(href="meet.google.com/kpx-bqzf-fzr").btn.btn-primary.mb-md-3.w-100
      | #[i.bi.bi-camera-reels-fill] Live Streaming
    a(href="https://goo.gl/maps/FL4qcbB3MnMXrYS28",target="_blank").btn.btn-primary.w-100
      | #[i.bi.bi-geo-alt-fill] Sala Demo
      