.row.mt-4.mb-4
  h2 #[span.emoji ⌛️] Past Talks

.row.project
  .col-md-2.col-3
    h3.mb-0 26th
    h5.month.mb-0 July
    p 15:00-16:00
  .col-md-7.col-9
    p.author #[span.me Gabriele Dominici and Pietro Barbiero]
    h4.title.mb-1
      | Causal Concept Embedding Models: Beyond Causal Opacity in Deep Learning
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-d0da46ee9bd422789dfadf91cd2ebc3c",aria-expanded="false",aria-controls="talk-d0da46ee9bd422789dfadf91cd2ebc3c")
        | #[i.bi.bi-file-earmark-text-fill] Abstract
      a(href="slides/Beyond the Cloud. Exploring Serverless Computing and Cloud Continuum.pdf",target="_blank").btn.btn-primary
        | #[i.bi.bi-easel3-fill] Slides
  .col-md-7
    p.abstract#talk-d0da46ee9bd422789dfadf91cd2ebc3c.collapse.mt-2
        | Causal opacity denotes the difficulty in understanding the hidden" causal structure underlying a deep neural network's (DNN) reasoning. This leads to the inability to rely on and verify state-of-the-art DNN-based systems especially in high-stakes scenarios. For this reason
.row.project
  .col-md-2.col-3
    h3.mb-0 10th
    h5.month.mb-0 July
    p 15:00-16:00
  .col-md-7.col-9
    p.author #[span.me Alessio Moneta]
    h4.title.mb-1
      | Vector Autoregressions: Identification and Causality
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-4b196a202ede1c2de1810c7a31fd267f",aria-expanded="false",aria-controls="talk-4b196a202ede1c2de1810c7a31fd267f")
        | #[i.bi.bi-file-earmark-text-fill] Abstract
      a(href="slides/Delivering solutions in ABR Video Streaming approaches and open Researches.pdf",target="_blank").btn.btn-primary
        | #[i.bi.bi-easel3-fill] Slides
  .col-md-7
    p.abstract#talk-4b196a202ede1c2de1810c7a31fd267f.collapse.mt-2
        | The structural vector autoregressive (SVAR) model is the main tool that macroeconomists use to address empirical questions about the effects of policy interventions. I will review the SVAR model, making comparisons with the structural causal model, and I will address the question whether it is equipped to warrant causal claims. Specifically, I will present the model identification problem and spot the difference with the problem of causal inference.
.row.project
  .col-md-2.col-3
    h3.mb-0 28th
    h5.month.mb-0 June
    p 15:00-16:00
  .col-md-7.col-9
    p.author #[span.me Jose Manuel Alvarez]
    h4.title.mb-1
      | Causal Perception
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-d59dd36c3a53130863c17d7ed8526d9a",aria-expanded="false",aria-controls="talk-d59dd36c3a53130863c17d7ed8526d9a")
        | #[i.bi.bi-file-earmark-text-fill] Abstract
      a(href="slides/Graph-based Approximate Nearest Neighbors Search.pdf",target="_blank").btn.btn-primary
        | #[i.bi.bi-easel3-fill] Slides
  .col-md-7
    p.abstract#talk-d59dd36c3a53130863c17d7ed8526d9a.collapse.mt-2
        | Perception occurs when two individuals interpret the same information differently. Despite being a known phenomenon with implications for bias in decision-making, as individual experience determines interpretation, perception remains largely overlooked in machine learning (ML) research. Modern decision flows, whether partially or fully automated, involve human experts interacting with ML applications. How might we then, e.g., account for two experts that interpret differently a deferred instance or an explanation from a ML model? To account for perception, we first need to formulate it. In this work, we define perception under causal reasoning using structural causal models (SCM). Our framework formalizes individual experience as additional causal knowledge that comes with and is used by a human expert (read, decision maker). We present two kinds of causal perception, unfaithful and inconsistent, based on the SCM properties of faithfulness and consistency. Further, we motivate the importance of perception within fairness problems. We illustrate our framework through a series of decision flow examples involving ML applications and human experts.
.row.project
  .col-md-2.col-3
    h3.mb-0 21st
    h5.month.mb-0 June
    p 14:00-15:00
  .col-md-7.col-9
    p.author #[span.me Riccardo Massidda]
    h4.title.mb-1
      | Learning Linear Causal Abstraction
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-8dd23f7b6fcee787bb63339ef1daaca1",aria-expanded="false",aria-controls="talk-8dd23f7b6fcee787bb63339ef1daaca1")
        | #[i.bi.bi-file-earmark-text-fill] Abstract
      a(href="slides/Maritime Traffic Monitoring Using Deep Learning.pdf",target="_blank").btn.btn-primary
        | #[i.bi.bi-easel3-fill] Slides
  .col-md-7
    p.abstract#talk-8dd23f7b6fcee787bb63339ef1daaca1.collapse.mt-2
        | Deferring systems extend supervised Machine Learning (ML) models with the possibility to defer predictions to human experts. However, evaluating the impact of a deferring strategy on system accuracy is still an overlooked area. This paper fills this gap by evaluating deferring systems through a causal lens. We link the potential outcomes framework for causal inference with deferring systems. This allows us to identify the causal impact of the deferring strategy on predictive accuracy. We distinguish two scenarios. In the first one, we can access both the human and the ML model predictions for the deferred instances. In such a case, we can identify the individual causal effects for deferred instances and aggregates of them. In the second scenario, only human predictions are available for the deferred instances. In this case, we can resort to regression discontinuity design to estimate a local causal effect. We empirically evaluate our approach on synthetic and real datasets for seven deferring systems from the literature.
.row.project
  .col-md-2.col-3
    h3.mb-0 7th
    h5.month.mb-0 June
    p 15:00-16:00
  .col-md-7.col-9
    p.author #[span.me Andrea Pugnana]
    h4.title.mb-1
      | A causal framework for evaluating deferring systems
    .btn-group.btn-group-sm(role="group",aria-label="Commands").mt-1
      
      button.btn.btn-primary(type="button",data-bs-toggle="collapse",data-bs-target="#talk-3b86636e11c2fbbdc93d8567f2ad8594",aria-expanded="false",aria-controls="talk-3b86636e11c2fbbdc93d8567f2ad8594")
        | #[i.bi.bi-file-earmark-text-fill] Abstract
      a(href="slides/Fun with strings Maximal Common Subsequences.pdf",target="_blank").btn.btn-primary
        | #[i.bi.bi-easel3-fill] Slides
  .col-md-7
    p.abstract#talk-3b86636e11c2fbbdc93d8567f2ad8594.collapse.mt-2
        | Deferring systems extend supervised Machine Learning (ML) models with the possibility to defer predictions to human experts. However, evaluating the impact of a deferring strategy on system accuracy is still an overlooked area. This paper fills this gap by evaluating deferring systems through a causal lens. We link the potential outcomes framework for causal inference with deferring systems. This allows us to identify the causal impact of the deferring strategy on predictive accuracy. We distinguish two scenarios. In the first one, we can access both the human and the ML model predictions for the deferred instances. In such a case, we can identify the individual causal effects for deferred instances and aggregates of them. In the second scenario, only human predictions are available for the deferred instances. In this case, we can resort to regression discontinuity design to estimate a local causal effect. We empirically evaluate our approach on synthetic and real datasets for seven deferring systems from the literature.