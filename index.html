<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Causal Club 2024</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>
    <link href="css/style.css" rel="stylesheet">
  </head>
  <body>
    <div class="container">
      <div class="row">
        <div class="col-md-5">
          <h1>Causal Club</h1>
          <h3>Reading Group 2024</h3>
        </div>
        <div class="col-md-7">
          <p> Join our reading group focused on 
            <mark>causality</mark> and related topics, organized by PhD students from the Department of Computer Science at the University of Pisa. Each session has a 
            <mark>presentation</mark> followed by a 
            <mark>panel discussion</mark>, providing a great opportunity to explore and debate key concepts in this field.
          </p>
          <p>For any further information, you can reach us out via <a href="mailto:causalclub@gmail.com">email</a>.</p>
        </div>
      </div>
      <div class="row mt-4 mb-2">
        <h2><span class="emoji">üöÄ</span> Upcoming</h2>
      </div>
      <div class="row next">
        <div class="col-md-7 col-8">
          <p class="author"><span class="me">Francesco Montagna</span></p>
          <h2 class="title mb-0 mt-1">Lessons on the identifiability of causal models: classical and supervised learning approaches</h2>
        </div>
        <div class="col-md-2 col-4">
          <h1 class="day">29th</h1>
          <h4 class="month mb-0">November</h4>
          <p>15:00-16:00</p>
        </div>
        <div class="col-md-7 col-12 mt-sm-3 mt-lg-1">
          <p>Causal discovery from observational data is hindered by restrictive modelling hypotheses that may be unrealistic and hard to verify. In this talk, we will illustrate how score matching estimation enables causal discovery with observational data on broad classes of causal models, with linear and nonlinear mechanisms, even in the presence of latent confounders. We draw the connection between our findings and the general principles that underlie most of the known results on the identifiability of causal models from observational data. Then, we turn our attention to supervised learning-based approaches for causal discovery: a plethora of learning algorithms for the amortized inference of causal graphs have been recently proposed, accompanied by impressive empirical results. Yet, it is unclear when the output of these methods can be trusted, as they do not come with clear identifiability guarantees. We show that amortized causal discovery still needs to obey identifiability theory, but it also differs from classical methods in how the assumptions are formulated, trading more reliance on assumptions on the noise type for fewer hypotheses on the mechanisms.</p>
        </div>
        <div class="col-md-4 d-grid gap-2 d-md-block"><a class="btn btn-primary mb-md-3 w-100" href="https://meet.google.com/sqw-xwtq-wci"><i class="bi bi-camera-reels-fill"></i> Live Streaming</a><a class="btn btn-primary w-100" href="https://goo.gl/maps/FL4qcbB3MnMXrYS28" target="_blank"><i class="bi bi-geo-alt-fill"></i> Sala Demo</a></div>
      </div>
      <div class="row mt-4 mb-4">
        <h2><span class="emoji">üîÆ</span> Next Talks</h2>
      </div>
      <div class="row project">
        <div class="col-md-2 col-3">
          <h3 class="mb-0">6th</h3>
          <h5 class="month mb-0">December</h5>
          <p>15:00-16:00</p>
        </div>
        <div class="col-md-7 col-9">
          <p class="author"><span class="me">Yorgos Felekis</span></p>
          <h4 class="title mb-1">Causal Optimal Transport of Abstractions</h4>
          <div class="btn-group btn-group-sm mt-1" role="group" aria-label="Commands">
            <button class="btn btn-primary" type="button" data-bs-toggle="collapse" data-bs-target="#talk-47a59c2589887c77ee1a6d0e1ee34d0f" aria-expanded="false" aria-controls="talk-47a59c2589887c77ee1a6d0e1ee34d0f"><i class="bi bi-card-text"></i> Abstract</button><a class="btn btn-primary" href="slides/Causal Optimal Transport of Abstractions.pdf" target="_blank"><i class="bi bi-file-earmark-text-fill"></i> Paper</a>
          </div>
        </div>
        <div class="col-md-7">
          <p class="abstract collapse mt-2" id="talk-47a59c2589887c77ee1a6d0e1ee34d0f">Causal abstraction (CA) theory establishes formal criteria for relating multiple structural causal models (SCMs) at different levels of granularity by defining maps between them. These maps have significant relevance for real-world challenges such as synthesizing causal evidence from multiple experimental environments learning causally consistent representations at different resolutions and linking interventions across multiple SCMs. In this talk Yorgos will present COTA, a method to learn abstraction maps from observational and interventional data without assuming complete knowledge of the underlying SCMs. In particular a multi-marginal Optimal Transport (OT) formulation was leveraged that enforces do-calculus causal constraints together with a cost function that relies on interventional information. COTA was thoroughly evaluated on both synthetic and real-world problems demonstrating clear advantages over non-causal independent and aggregated OT formulations. Additionally the method's efficiency was showcased as a data augmentation tool by comparing it to the state-of-the-art CA learning framework which assumes fully specified SCMs on a real-world downstream task.</p>
        </div>
      </div>
      <div class="row mt-4 mb-4">
        <h2><span class="emoji">‚åõÔ∏è</span> Past Talks</h2>
      </div>
      <div class="row project">
        <div class="col-md-2 col-3">
          <h3 class="mb-0">22nd</h3>
          <h5 class="month mb-0">November</h5>
          <p>15:00-16:00</p>
        </div>
        <div class="col-md-7 col-9">
          <p class="author"><span class="me">Emanuale Marconato</span></p>
          <h4 class="title mb-1">A Causal framework for interpreting model representations</h4>
          <div class="btn-group btn-group-sm mt-1" role="group" aria-label="Commands">
            <button class="btn btn-primary" type="button" data-bs-toggle="collapse" data-bs-target="#talk-6067ba3aca8715ebb6027fd3b65fcf79" aria-expanded="false" aria-controls="talk-6067ba3aca8715ebb6027fd3b65fcf79"><i class="bi bi-card-text"></i> Abstract</button>
          </div>
        </div>
        <div class="col-md-7">
          <p class="abstract collapse mt-2" id="talk-6067ba3aca8715ebb6027fd3b65fcf79">There is a growing interest in providing explanations of machine learning models using human-understandable concepts. This integration of concepts can alleviate model opacity due to neural networks opening new venues to design interpretable and trustworthy models. However a critical problem is ensuring that machine learning models successfully encode the human concepts in their representations in a correct manner. Specifically when can we assign names to machine representations and how well do their concepts align with ours? I will demonstrate how Causality and Causal Representation Learning can help formalize this problem offering a clear definition of what can be referred to as human-machine alignment. Throughout the presentation I will showcase examples of both successes and failures in models designed to integrate concepts into their representations.</p>
        </div>
      </div>
      <div class="row project">
        <div class="col-md-2 col-3">
          <h3 class="mb-0">15th</h3>
          <h5 class="month mb-0">November</h5>
          <p>15:00-16:00</p>
        </div>
        <div class="col-md-7 col-9">
          <p class="author"><span class="me">Alexander Reisach</span></p>
          <h4 class="title mb-1">Sortability in Additive Noise Models</h4>
          <div class="btn-group btn-group-sm mt-1" role="group" aria-label="Commands">
            <button class="btn btn-primary" type="button" data-bs-toggle="collapse" data-bs-target="#talk-5d315bd4dfe6826ee76219cbfcb279c9" aria-expanded="false" aria-controls="talk-5d315bd4dfe6826ee76219cbfcb279c9"><i class="bi bi-card-text"></i> Abstract</button><a class="btn btn-primary" href="slides/Sortability in Additive Noise Models.pdf" target="_blank"><i class="bi bi-file-earmark-text-fill"></i> Paper</a>
          </div>
        </div>
        <div class="col-md-7">
          <p class="abstract collapse mt-2" id="talk-5d315bd4dfe6826ee76219cbfcb279c9">Causal graphical models encode causal relationships between variables, typically based on a directed acyclic graph (DAG). Structural causal models expand upon this by expressing the causal relationships as explicit functions, which allows for computing the effect of interventions and much more. We show that, in many common parameterizations using linear functions and additive noise, effects accumulate along the causal order. This property enables inferring the causal order from data simply by sorting by a suitable criterion. We introduce the concept of sortability to capture the magnitude of the phenomenon, and show that the accumulating effects can lead to unrealistic values and deterministic relationships. We discuss implications for existing results and future research based on the same model class.</p>
        </div>
      </div>
      <div class="row project">
        <div class="col-md-2 col-3">
          <h3 class="mb-0">8th</h3>
          <h5 class="month mb-0">November</h5>
          <p>15:00-16:00</p>
        </div>
        <div class="col-md-7 col-9">
          <p class="author"><span class="me">Junhyung Park</span></p>
          <h4 class="title mb-1">Causal Spaces A measure-theoretic axiomatisation of causality</h4>
          <div class="btn-group btn-group-sm mt-1" role="group" aria-label="Commands">
            <button class="btn btn-primary" type="button" data-bs-toggle="collapse" data-bs-target="#talk-94ec16d11c0b9c8de0fad620176ed030" aria-expanded="false" aria-controls="talk-94ec16d11c0b9c8de0fad620176ed030"><i class="bi bi-card-text"></i> Abstract</button><a class="btn btn-primary" href="slides/Causal Spaces A measure-theoretic axiomatisation of causality.pdf" target="_blank"><i class="bi bi-file-earmark-text-fill"></i> Paper</a>
          </div>
        </div>
        <div class="col-md-7">
          <p class="abstract collapse mt-2" id="talk-94ec16d11c0b9c8de0fad620176ed030">We view causality both as an extension of probability theory and as a study of what happens when one intervenes on a system, and argue in favour of taking Kolmogorov's measure-theoretic axiomatisation of probability as the starting point towards an axiomatisation of causality. To that end, we propose the notion of a causal space, consisting of a probability space along with a collection of transition probability kernels, called causal kernels, that encode the causal information of the space. Our proposed framework is not only rigorously grounded in measure theory, but it also sheds light on long-standing limitations of existing frameworks including, for example, cycles, latent variables and stochastic processes.</p>
        </div>
      </div>
      <div class="row project">
        <div class="col-md-2 col-3">
          <h3 class="mb-0">25th</h3>
          <h5 class="month mb-0">October</h5>
          <p>15:00-16:00</p>
        </div>
        <div class="col-md-7 col-9">
          <p class="author"><span class="me">Jacopo Bono</span></p>
          <h4 class="title mb-1">DiConStruct Causal Concept-based Explanations through Black-Box Distillation</h4>
          <div class="btn-group btn-group-sm mt-1" role="group" aria-label="Commands">
            <button class="btn btn-primary" type="button" data-bs-toggle="collapse" data-bs-target="#talk-734d80f7185e98ffc58a5c66d27b4907" aria-expanded="false" aria-controls="talk-734d80f7185e98ffc58a5c66d27b4907"><i class="bi bi-card-text"></i> Abstract</button><a class="btn btn-primary" href="slides/DiConStruct Causal Concept-based Explanations through Black-Box Distillation.pdf" target="_blank"><i class="bi bi-file-earmark-text-fill"></i> Paper</a>
          </div>
        </div>
        <div class="col-md-7">
          <p class="abstract collapse mt-2" id="talk-734d80f7185e98ffc58a5c66d27b4907">Model interpretability plays a central role in human-AI decision-making systems. Ideally, explanations should be expressed using human-interpretable semantic concepts. Moreover, the causal relations between these concepts should be captured by the explainer to allow for reasoning about the explanations. Lastly, explanation methods should be efficient and not compromise the predictive task performance. Despite the recent rapid advances in AI explainability, as far as we know, no method yet fulfills these three desiderata. Indeed, mainstream methods for local concept explainability do not yield causal explanations and incur a trade-off between explainability and prediction accuracy. We present DiConStruct, an explanation method that is both concept-based and causal, which produces more interpretable local explanations in the form of structural causal models and concept attributions. Our explainer works as a distillation model to any black-box machine learning model by approximating its predictions while producing the respective explanations. Consequently, DiConStruct generates explanations efficiently while not impacting the black-box prediction task. We validate our method on an image dataset and a tabular dataset, showing that DiConStruct approximates the black-box models with higher fidelity than other concept explainability baselines, while providing explanations that include the causal relations between the concepts.</p>
        </div>
      </div>
      <div class="row project">
        <div class="col-md-2 col-3">
          <h3 class="mb-0">18th</h3>
          <h5 class="month mb-0">October</h5>
          <p>15:00-16:00</p>
        </div>
        <div class="col-md-7 col-9">
          <p class="author"><span class="me">Alberto Caron</span></p>
          <h4 class="title mb-1">Causal Active Planning in Model-Based Reinforcement Learning</h4>
          <div class="btn-group btn-group-sm mt-1" role="group" aria-label="Commands">
            <button class="btn btn-primary" type="button" data-bs-toggle="collapse" data-bs-target="#talk-40f4f4662f9022ee542d7df917ef4bdd" aria-expanded="false" aria-controls="talk-40f4f4662f9022ee542d7df917ef4bdd"><i class="bi bi-card-text"></i> Abstract</button>
          </div>
        </div>
        <div class="col-md-7">
          <p class="abstract collapse mt-2" id="talk-40f4f4662f9022ee542d7df917ef4bdd">In this talk, I am going to present some results from our latest paper on the problem of sample efficient exploration in Model-Based Reinforcement Learning (MBRL). While most popular exploration methods in (MB)RL are ‚Äúreactive‚Äù in nature, and thus inherently sample inefficient, we discuss the benefits of an ‚Äúactive‚Äù approach, where the agent selects actions to query novel states in a data-efficient way, provided that one can guarantee that regions of high epistemic, and not aleatoric, uncertainty are targeted. In order to ensure this, we consider classes of exploration bonuses based on ‚ÄúBayesian surprise‚Äù, and demonstrate their desirable properties. The rest of the talk will be dedicated to presenting our latest, early stage, project that aims at extending this previous work to develop a ‚ÄúCausal Active Planning‚Äù framework for sample efficient and robust MBRL, where the goal is to expand this class of exploration bonuses to capture epistemic uncertainty around a (Local) Causal Graphical Model of the transition dynamics. By incorporating these measures as causal ‚Äúintrinsic rewards‚Äù within a MBRL algorithm, we can enable the agent to (actively) learn a causal transition dynamics model. If this causal dynamics model is then consequently utilized for policy training, it should theoretically ensure that the learnt policy is more robust and generalizable (Richens & Everitt, 2024).</p>
        </div>
      </div>
      <div class="row project">
        <div class="col-md-2 col-3">
          <h3 class="mb-0">11th</h3>
          <h5 class="month mb-0">October</h5>
          <p>15:00-16:00</p>
        </div>
        <div class="col-md-7 col-9">
          <p class="author"><span class="me">Isacco Beretta and Martina Cinquini</span></p>
          <h4 class="title mb-1">A Practical Approach to Causal Inference over Time</h4>
          <div class="btn-group btn-group-sm mt-1" role="group" aria-label="Commands">
            <button class="btn btn-primary" type="button" data-bs-toggle="collapse" data-bs-target="#talk-4323033ddb0c0f1c6d4e2f6b9b3a3c78" aria-expanded="false" aria-controls="talk-4323033ddb0c0f1c6d4e2f6b9b3a3c78"><i class="bi bi-card-text"></i> Abstract</button><a class="btn btn-primary" href="slides/A Practical Approach to Causal Inference over Time.pdf" target="_blank"><i class="bi bi-file-earmark-text-fill"></i> Paper</a>
          </div>
        </div>
        <div class="col-md-7">
          <p class="abstract collapse mt-2" id="talk-4323033ddb0c0f1c6d4e2f6b9b3a3c78">In this paper, we focus on estimating the causal effect of an intervention over time on a dynamical system. To that end, we formally define causal interventions and their effects over time on discrete-time stochastic processes (DSPs). Then, we show under which conditions the equilibrium states of a DSP, both before and after a causal intervention, can be captured by a structural causal model (SCM). With such an equivalence at hand, we provide an explicit mapping from vector autoregressive models (VARs), broadly applied in econometrics, to linear, but potentially cyclic and/or affected by unmeasured confounders, SCMs. The resulting causal VAR framework allows us to perform causal inference over time from observational time series data. Our experiments on synthetic and real-world datasets show that the proposed framework achieves strong performance in terms of observational forecasting while enabling accurate estimation of the causal effect of interventions on dynamical systems. We demonstrate, through a case study, the potential practical questions that can be addressed using the proposed causal VAR framework.</p>
        </div>
      </div>
      <div class="row project">
        <div class="col-md-2 col-3">
          <h3 class="mb-0">26th</h3>
          <h5 class="month mb-0">July</h5>
          <p>15:00-16:00</p>
        </div>
        <div class="col-md-7 col-9">
          <p class="author"><span class="me">Gabriele Dominici and Pietro Barbiero</span></p>
          <h4 class="title mb-1">Causal Concept Embedding Models Beyond Causal Opacity in Deep Learning</h4>
          <div class="btn-group btn-group-sm mt-1" role="group" aria-label="Commands">
            <button class="btn btn-primary" type="button" data-bs-toggle="collapse" data-bs-target="#talk-f391642789f7d783d15e99dcfed93395" aria-expanded="false" aria-controls="talk-f391642789f7d783d15e99dcfed93395"><i class="bi bi-card-text"></i> Abstract</button><a class="btn btn-primary" href="slides/Causal Concept Embedding Models Beyond Causal Opacity in Deep Learning.pdf" target="_blank"><i class="bi bi-file-earmark-text-fill"></i> Paper</a>
          </div>
        </div>
        <div class="col-md-7">
          <p class="abstract collapse mt-2" id="talk-f391642789f7d783d15e99dcfed93395">Causal opacity denotes the difficulty in understanding the hidden causal structure underlying a deep neural network's (DNN) reasoning. This leads to the inability to rely on and verify state-of-the-art DNN-based systems especially in high-stakes scenarios. For this reason, causal opacity represents a key open challenge at the intersection of deep learning, interpretability, and causality. This work addresses this gap by introducing Causal Concept Embedding Models (Causal CEMs), a class of interpretable models whose decision-making process is causally transparent by design. The results of our experiments show that Causal CEMs can: (i) match the generalization performance of causally-opaque models, (ii) support the analysis of interventional and counterfactual scenarios, thereby improving the model's causal interpretability and supporting the effective verification of its reliability and fairness, and (iii) enable human-in-the-loop corrections to mispredicted intermediate reasoning steps, boosting not just downstream accuracy after corrections but also accuracy of the explanation provided for a specific instance.</p>
        </div>
      </div>
      <div class="row project">
        <div class="col-md-2 col-3">
          <h3 class="mb-0">10th</h3>
          <h5 class="month mb-0">July</h5>
          <p>14:00-15:00</p>
        </div>
        <div class="col-md-7 col-9">
          <p class="author"><span class="me">Alessio Moneta</span></p>
          <h4 class="title mb-1">Vector Autoregressions Identification and Causality</h4>
          <div class="btn-group btn-group-sm mt-1" role="group" aria-label="Commands">
            <button class="btn btn-primary" type="button" data-bs-toggle="collapse" data-bs-target="#talk-7e3a68d4457bf2f02c2ed2392b81fe26" aria-expanded="false" aria-controls="talk-7e3a68d4457bf2f02c2ed2392b81fe26"><i class="bi bi-card-text"></i> Abstract</button><a class="btn btn-primary" href="slides/Vector Autoregressions Identification and Causality.pdf" target="_blank"><i class="bi bi-file-earmark-text-fill"></i> Paper</a>
          </div>
        </div>
        <div class="col-md-7">
          <p class="abstract collapse mt-2" id="talk-7e3a68d4457bf2f02c2ed2392b81fe26">The structural vector autoregressive (SVAR) model is the main tool that macroeconomists use to address empirical questions about the effects of policy interventions. I will review the SVAR model, making comparisons with the structural causal model, and I will address the question whether it is equipped to warrant causal claims. Specifically, I will present the model identification problem and spot the difference with the problem of causal inference.</p>
        </div>
      </div>
      <div class="row project">
        <div class="col-md-2 col-3">
          <h3 class="mb-0">28th</h3>
          <h5 class="month mb-0">June</h5>
          <p>15:00-16:00</p>
        </div>
        <div class="col-md-7 col-9">
          <p class="author"><span class="me">Jose Manuel Alvarez</span></p>
          <h4 class="title mb-1">Causal Perception</h4>
          <div class="btn-group btn-group-sm mt-1" role="group" aria-label="Commands">
            <button class="btn btn-primary" type="button" data-bs-toggle="collapse" data-bs-target="#talk-6e2c97249723e313dd3c2449532a9ba4" aria-expanded="false" aria-controls="talk-6e2c97249723e313dd3c2449532a9ba4"><i class="bi bi-card-text"></i> Abstract</button><a class="btn btn-primary" href="slides/Causal Perception.pdf" target="_blank"><i class="bi bi-file-earmark-text-fill"></i> Paper</a>
          </div>
        </div>
        <div class="col-md-7">
          <p class="abstract collapse mt-2" id="talk-6e2c97249723e313dd3c2449532a9ba4">Perception occurs when two individuals interpret the same information differently. Despite being a known phenomenon with implications for bias in decision-making, as individual experience determines interpretation, perception remains largely overlooked in machine learning (ML) research. Modern decision flows, whether partially or fully automated, involve human experts interacting with ML applications. How might we then, e.g., account for two experts that interpret differently a deferred instance or an explanation from a ML model? To account for perception, we first need to formulate it. In this work, we define perception under causal reasoning using structural causal models (SCM). Our framework formalizes individual experience as additional causal knowledge that comes with and is used by a human expert (read, decision maker). We present two kinds of causal perception, unfaithful and inconsistent, based on the SCM properties of faithfulness and consistency. Further, we motivate the importance of perception within fairness problems. We illustrate our framework through a series of decision flow examples involving ML applications and human experts.</p>
        </div>
      </div>
      <div class="row project">
        <div class="col-md-2 col-3">
          <h3 class="mb-0">21st</h3>
          <h5 class="month mb-0">June</h5>
          <p>15:00-16:00</p>
        </div>
        <div class="col-md-7 col-9">
          <p class="author"><span class="me">Riccardo Massidda</span></p>
          <h4 class="title mb-1">Learning Linear Causal Abstraction</h4>
          <div class="btn-group btn-group-sm mt-1" role="group" aria-label="Commands">
            <button class="btn btn-primary" type="button" data-bs-toggle="collapse" data-bs-target="#talk-89ee1ad65c602ab48f5110f2fe9e267e" aria-expanded="false" aria-controls="talk-89ee1ad65c602ab48f5110f2fe9e267e"><i class="bi bi-card-text"></i> Abstract</button><a class="btn btn-primary" href="slides/Learning Linear Causal Abstraction.pdf" target="_blank"><i class="bi bi-file-earmark-text-fill"></i> Paper</a>
          </div>
        </div>
        <div class="col-md-7">
          <p class="abstract collapse mt-2" id="talk-89ee1ad65c602ab48f5110f2fe9e267e">The need for modelling causal knowledge at different levels of granularity arises in several settings. Causal Abstraction provides a framework for formalizing this problem by relating two Structural Causal Models at different levels of detail. Despite increasing interest in applying causal abstraction, e.g. in the interpretability of large machine learning models, the graphical and parametrical conditions under which a causal model can abstract another are not known. Furthermore, learning causal abstractions from data is still an open problem. In this work, we tackle both issues for linear causal models with linear abstraction functions. First, we characterize how the low-level coefficients and the abstraction function determine the high-level coefficients and how the high-level model constrains the causal ordering of low-level variables. Then, we apply our theoretical results to learn high-level and low-level causal models and their abstraction function from observational data. In particular, we introduce Abs-LiNGAM, a method that leverages the constraints induced by the learned high-level model and the abstraction function to speedup the recovery of the larger low-level model, under the assumption of non-Gaussian noise terms. In simulated settings, we show the effectiveness of learning causal abstractions from data and the potential of our method in improving scalability of causal discovery.</p>
        </div>
      </div>
      <div class="row project">
        <div class="col-md-2 col-3">
          <h3 class="mb-0">7th</h3>
          <h5 class="month mb-0">June</h5>
          <p>15:00-16:00</p>
        </div>
        <div class="col-md-7 col-9">
          <p class="author"><span class="me">Andrea Pugnana</span></p>
          <h4 class="title mb-1">A causal framework for evaluating deferring systems</h4>
          <div class="btn-group btn-group-sm mt-1" role="group" aria-label="Commands">
            <button class="btn btn-primary" type="button" data-bs-toggle="collapse" data-bs-target="#talk-15ae137ff1f0d3ecbd01cf076cdcf5e4" aria-expanded="false" aria-controls="talk-15ae137ff1f0d3ecbd01cf076cdcf5e4"><i class="bi bi-card-text"></i> Abstract</button><a class="btn btn-primary" href="slides/A causal framework for evaluating deferring systems.pdf" target="_blank"><i class="bi bi-file-earmark-text-fill"></i> Paper</a>
          </div>
        </div>
        <div class="col-md-7">
          <p class="abstract collapse mt-2" id="talk-15ae137ff1f0d3ecbd01cf076cdcf5e4">Deferring systems extend supervised Machine Learning (ML) models with the possibility to defer predictions to human experts. However, evaluating the impact of a deferring strategy on system accuracy is still an overlooked area. This paper fills this gap by evaluating deferring systems through a causal lens. We link the potential outcomes framework for causal inference with deferring systems. This allows us to identify the causal impact of the deferring strategy on predictive accuracy. We distinguish two scenarios. In the first one, we can access both the human and the ML model predictions for the deferred instances. In such a case, we can identify the individual causal effects for deferred instances and aggregates of them. In the second scenario, only human predictions are available for the deferred instances. In this case, we can resort to regression discontinuity design to estimate a local causal effect. We empirically evaluate our approach on synthetic and real datasets for seven deferring systems from the literature.</p>
        </div>
      </div>
    </div>
  </body>
</html>