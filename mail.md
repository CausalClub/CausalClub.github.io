Subject: [Seminar] DiConStruct Causal Concept-based Explanations through Black-Box Distillation

Dear Causal Clubbers,

We are pleased to invite you to our upcoming seminar:

**Presenter:** Jacopo Bono (Feedzai)

**When:** 25/10/2024, 15:00-16:00 [[Add to Calendar](https://www.google.com/calendar/render?action=TEMPLATE&text=Seminar%3A%20DiConStruct%20Causal%20Concept-based%20Explanations%20through%20Black-Box%20Distillation&dates=20241025T150000/20241025T160000&details=Presenter%3A%20Jacopo%20Bono%20%28Feedzai%29%0A%0A%20Join%20Teams%20Meeting%3A%20meet.google.com/avt-sjkt-jzb&location=Sala%20Demo)]

**Where:** Sala Demo, Computer Science Department, University of Pisa

**To join online:** meet.google.com/avt-sjkt-jzb

**Title:** DiConStruct Causal Concept-based Explanations through Black-Box Distillation

**Abstract:** 
Model interpretability plays a central role in human-AI decision-making systems. Ideally, explanations should be expressed using human-interpretable semantic concepts. Moreover, the causal relations between these concepts should be captured by the explainer to allow for reasoning about the explanations. Lastly, explanation methods should be efficient and not compromise the predictive task performance. Despite the recent rapid advances in AI explainability, as far as we know, no method yet fulfills these three desiderata. Indeed, mainstream methods for local concept explainability do not yield causal explanations and incur a trade-off between explainability and prediction accuracy. We present DiConStruct, an explanation method that is both concept-based and causal, which produces more interpretable local explanations in the form of structural causal models and concept attributions. Our explainer works as a distillation model to any black-box machine learning model by approximating its predictions while producing the respective explanations. Consequently, DiConStruct generates explanations efficiently while not impacting the black-box prediction task. We validate our method on an image dataset and a tabular dataset, showing that DiConStruct approximates the black-box models with higher fidelity than other concept explainability baselines, while providing explanations that include the causal relations between the concepts.

We look forward to seeing you at the seminar!

Best,

Causal Club 
