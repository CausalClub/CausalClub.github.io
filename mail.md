Subject: [Seminar] Causal Active Planning in Model-Based Reinforcement Learning

Dear Causal Clubbers,

We are pleased to invite you to our upcoming seminar:

**Presenter:** Alberto Caron (The Alan Turing Institute)

**When:** 18/10/2024, 15:00-16:00 [[Add to Calendar](https://www.google.com/calendar/render?action=TEMPLATE&text=Seminar%3A%20Causal%20Active%20Planning%20in%20Model-Based%20Reinforcement%20Learning&dates=20241018T150000/20241018T160000&details=Presenter%3A%20Alberto%20Caron%20%28The%20Alan%20Turing%20Institute%29%0A%0A%20Join%20Teams%20Meeting%3A%20meet.google.com/kpx-bqzf-fzr&location=Sala%20Demo)]

**Where:** Sala Demo, Computer Science Department, University of Pisa

**To join online:** meet.google.com/kpx-bqzf-fzr

**Title:** Causal Active Planning in Model-Based Reinforcement Learning

**Abstract:** 
In this talk, I am going to present some results from our latest paper on the problem of sample efficient exploration in Model-Based Reinforcement Learning (MBRL). While most popular exploration methods in (MB)RL are “reactive” in nature, and thus inherently sample inefficient, we discuss the benefits of an “active” approach, where the agent selects actions to query novel states in a data-efficient way, provided that one can guarantee that regions of high epistemic, and not aleatoric, uncertainty are targeted. In order to ensure this, we consider classes of exploration bonuses based on “Bayesian surprise”, and demonstrate their desirable properties. The rest of the talk will be dedicated to presenting our latest, early stage, project that aims at extending this previous work to develop a “Causal Active Planning” framework for sample efficient and robust MBRL, where the goal is to expand this class of exploration bonuses to capture epistemic uncertainty around a (Local) Causal Graphical Model of the transition dynamics. By incorporating these measures as causal “intrinsic rewards” within a MBRL algorithm, we can enable the agent to (actively) learn a causal transition dynamics model. If this causal dynamics model is then consequently utilized for policy training, it should theoretically ensure that the learnt policy is more robust and generalizable (Richens & Everitt, 2024).

We look forward to seeing you at the seminar!

Best,

Causal Club 
